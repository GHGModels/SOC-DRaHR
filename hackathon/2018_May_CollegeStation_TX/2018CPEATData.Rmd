---
title: "Data from C-PEAT"
author: "C-PEAT and Texas A&M Hackers"
date: "5/7/2018"
output: 
  html_document: 
    toc: yes
---

# Set Up
```{r setup}
library(tidyverse)

#mapping libraries to help with global/regional plots
library(ggmap)
library(mapdata)
library(fiftystater)

```

```{r download}
data.dir <- '~/Documents/Datasets/Data Hackathon TAMU 2018'
#This will be filled in later when we have the data DOI links assigned
```

```{r eval=FALSE}
library(googledrive)
##pull all the files that we are working with for the hacakthon from the google drive
file.ls <- drive_ls(path='Professional/Data Hackathon TAMU 2018')
cat(sprintf('# %s \n\n', sort(file.ls$name)))
```

```{r readInCPEAT2018May}

readInCPEAT2018May <- function(filename, #where is the download
                               metaCol=1:2, #what rows define the metadata for the core
                               data01Col=5:14, #row index of non-isotope data for the core
                               data01nonNum=c('peat_type'), #what is not a number in the non-isotope
                               data02Col=17:23, #row index of isotope data
                               debug=FALSE){ #pass through a copy of the raw data read
  
  #big data table of everything without headers
  allData <- read_csv(file=filename, skip=1, col_names=FALSE) 
  
  #initalize the list we'll pass the answer to
  ans <- list()
  
  #append the raw read if we are in debug mode
  if(debug){
    ans$rawRead <- allData
  }
  
  #There are no headers
  ans$site <- allData[,metaCol] #read first two columns as metadata
  names(ans$site) <- c('X1', 'X2') #force namings
  
  ans$site <- ans$site %>%
    filter(!is.na(X1) & !grepl('^\\s*$', X1)) %>% #remove empty rows
    spread(key=X1, value=X2) #convert table from long to wide
  
  #the rest of the data has a header, rename the columns
  names(allData) <- allData[1,]
  allData <- allData[-1,]
  
  #construct a table with the the depths
  depthTable <- allData[, data01Col] %>% 
    select(depth) %>% filter(!is.na(depth)) %>% 
    mutate(layer_mid=as.numeric(depth),
           layer_top=as.numeric(NA),
           layer_bottom=as.numeric(NA)) %>%
    arrange(layer_mid)
  
  depthTable$layer_top[1] <- 0
  depthTable$layer_bottom[1] <- depthTable$layer_mid[1]*2
  for(ii in 2:nrow(depthTable)){
    depthTable$layer_top[ii] <- depthTable$layer_bottom[ii-1]
    depthTable$layer_bottom[ii] <- (depthTable$layer_mid[ii]-depthTable$layer_top[ii]) + depthTable$layer_mid[ii]
  }
  
  ##Pull the non-isotope data table
  depthID <- c('depth', data01nonNum)
  sampleData01 <- allData[, data01Col] %>%
    gather(key='header', value='value', -one_of(depthID)) %>%
    #gather(key='header', value='value', -depth, -peat_type) %>% #convert to long format exclude columns that are characters not numerics
    filter(!is.na(value) & !grepl('^\\s*$', value)) %>% #remove missing values
    mutate(value=as.numeric(value)) %>% #make sure values are numerics
    left_join(depthTable, by='depth') %>%
    mutate(layer_name=paste(ans$site$core_name, depth, sep='_'))
  
  ##Pull the isotope data table
  sampleData02 <- allData[ ,data02Col] %>% #identify columns for age
    rename(header='date_type', 
           value='uncal_date_BP', 
           sigma='lab_uncertainty_yrs',
           depth='depth_cm') %>% #harmonize header names to match previous samples
    filter(!is.na(value) & !grepl('^\\s*$', value)) %>% #remove missing values
    mutate(unit=if_else(grepl('\\D', value), 'non standard', 'date_BP'),
           layer_name=paste(ans$site$core_name, depth, sep='_'),
           layer_mid=as.numeric(depth),
           layer_top=as.numeric(depth)-as.numeric(thickness_cm)/2,
           layer_bottom=as.numeric(depth)+as.numeric(thickness_cm)/2) %>%
    mutate(value=gsub('\\D+', '', value)) %>% #break out so that we don't confuse unit def
    group_by(layer_name, header, layer_top, layer_bottom, value, sigma, unit) %>%
    gather(key='key', value='text', material, labe_code) %>%
    summarize(method = paste0(sprintf('%s: %s', key, text), collapse='; ')) %>%
    ungroup() %>%
    mutate(value=as.numeric(value),
           sigma=as.numeric(sigma))
  
  ##Construct the layer description to add to wide table
  layerID <- c('layer_name', 'layer_top', 'layer_bottom', data01nonNum)
  layer_description <- sampleData01 %>% ungroup() %>%
    select(one_of(layerID)) %>%
    full_join(sampleData02 %>% ungroup ()%>%
                select(layer_name, layer_top, layer_bottom), 
              by=c("layer_name", "layer_top", "layer_bottom")) %>%
    arrange(layer_top) %>% unique()
  
  ans$layer <- data.frame(layer_description, ans$site %>% select(site_name))
  
  ##construct long data table
  ans$sample <- sampleData01 %>%
    bind_rows(sampleData02) %>% #combine with previous samples
    select(layer_name, header, value, sigma, unit)
  
  
  return(ans)
}

temp <- readInCPEAT2018May(filename=file.path(data.dir, "Covey_Hill.csv"))
```

# Start here

1) Fork the main repository https://github.com/ISCN/SOC-DRaHR
2) Start by going to the master file and claiming a data file to work on. https://docs.google.com/spreadsheets/d/1hXyCW5TkLrt7en7tcz43lWHPxl5vVtG9qlkcr6t3zBs/edit?usp=sharing
3) Download your claimed data file from the google drive to a tempoary directory that you DO NOT commit to the repository https://drive.google.com/open?id=1k7CwjDePBuounwCMU1UFcWEsBlvwReWt
    - Do not commit data to the repository. The pull request will be denied.
4) Hack!
    1) Read in each table on the sheet
    2) If data is a soil measurement convert to a long table, otherwise keep wide
        - make sure entries are cross referenced and try to use informative unique ids
    3) Convert soil depth to top/bottom depth
    4) Convert to ISCN naming scheme and seperate units; see CPEAT_key.csv
    5) Plot data in histograms and map lat/lon; eyeball to make sure everything looks right
5) Add to your local git repository and push to your remote repository
6) Submit a pull request to the main repository
7) Claim your contributor status!
8) Repeat 1-6 until no data left

Remember to:

1) Steal shamelessly and credit generously.
2) Ask Google; then ask your neighbor; then ask an 'expert'.
3) Celebrate new and interesting mistakes.
4) There is ALWAYS more then one way to do something.
5) Document your code like you're passing it onto a dear friend to maintain.

## Useful notes
Any useful notes you find can go here for now: https://docs.google.com/document/d/1WeqesuFO--5AhQHQywNzdYSIoR9dctklhLjmCZy1chk/edit?usp=sharing
They will be transcribed to this document after the hackathon.

# Data ingest scripts

```{r loadFiles, message=FALSE}

allFiles <- c( "86-Kvartal.csv", "Aero.csv", 
  "Altay.csv", "Bear.csv", "Burnt_Village.csv", "Covey_Hill.csv", #8
  "D127.csv", "E110.csv", "Ennadai.csv", "Glen_Carron.csv", #12
  "Glen_Torridon.csv", "Goldeye.csv", "HL02.csv", "Hongyuan.csv", #16
  "Horse_Trail.csv", "JBL1.csv", "JBL2.csv", "JBL3.csv", #20
  "JBL4.csv", "JBL5.csv", "JBL7.csv", "JBL8.csv", #24
  "Joey.csv", "KAM12-C1.csv", "KAM12-C4.csv", "Kenai_Gasfield.csv", #28
  "KJ2-3.csv", "KUJU.csv", "La_Grande2.csv", "La_Grande3.csv", #32
  "Lac_Le_Caron.csv", "Lake396.csv", "Lake785.csv", "Lebel.csv", #36
  "Lompolojankka.csv", "Mariana.csv", "Martin.csv", "Mosaik.csv", #40           
  "No_Name_Creek.csv", "Nuikluk.csv", "NW-BG.csv", "Ours.csv", #44
  "Patuanak.csv", "Petersville.csv", "Petite_Bog.csv", "Plaine.csv", #48
  "Rogovaya.csv", "Saarisuo.csv", "Selwyn.csv", "Shuttle.csv", #52
  "SIB06.csv", "Sidney.csv", "Siikaneva.csv", "Slave.csv", #56
  "Sterne.csv", "Stordalen.csv", "Sundance.csv", "Swanson.csv", #60
  "T1.csv", "Unit.csv", "Upper_Pinto.csv", "Usinsk.csv", #64
  "Utikuma.csv", "V34.csv", "Vasyugan.csv", "VC04-06.csv", #68
  "Zoige.csv")

missingPeat_type <- c('Lebel.csv', 'Plaine.csv')
badCasting <- c('Selwyn.csv', 'Upper_Pinto.csv')

multiple_cores <- c('Joey.csv'=6, 'Mariana.csv'=3, 'NW-BG.csv'=4, 'Ours.csv'=3, 
                    'Nuikluk.csv'=2, "Rogovaya.csv"=2, "Sundance.csv"=2)

ans <- list(sample=data.frame(), layer=data.frame(), site=data.frame())
for(rootname in allFiles){
  #print(rootname)
  if(rootname %in% names(multiple_cores)){
    numCores <- multiple_cores[rootname]
  }else{
    numCores <- 1
  }
  
  if(rootname %in% missingPeat_type){
    nonNumHeader <- c()
    secondTable <- 5:13
    thirdTable <- 16:22
  }else{
    nonNumHeader <- c("peat_type")
    secondTable <- 5:14
    thirdTable <- 17:23
  }
  
  for(offset in (1:numCores)-1){
      temp <- readInCPEAT2018May(filename=file.path(data.dir, rootname),
                                 metaCol=c(1, 2+offset),
                                 data01Col = secondTable+numCores-1, data01nonNum = nonNumHeader, 
                                 data02Col = thirdTable+numCores-1, debug=TRUE)
    
    
    ans$sample <- ans$sample %>%
      bind_rows(temp$sample)
    ans$layer <- ans$layer %>%
      bind_rows(temp$layer)
    ans$site <- ans$site %>%
      bind_rows(temp$site)
  }
}
```

```{r reportTrouble}
##Recode bad C14 header
ans$sample <- ans$sample %>% mutate(header = recode(header, C14 = '14C'))

## TODO Sort out non-standard dates
ans$sample %>% filter(unit == 'non standard') %>% print
## TODO Sort out bad uncertainty entries in badCasting <- c('Selwyn.csv', 'Upper_Pinto.csv')
```

```{r plotEverything}
plot.df <- ans$sample %>%
           left_join(ans$layer %>% select(layer_name, site_name), by='layer_name')

for(oneSite in ans$site$site_name[1]){

  print(ggplot(plot.df %>% filter(site_name==oneSite)) +
          #geom_histogram(data=plot.df %>% sample_n(sum(plot.df$site_name==oneSite)),
          #               aes(x=value), fill='grey') +
          geom_histogram(aes(x=value)) +
          facet_wrap(~header+unit, scales='free') +
          labs(title=oneSite)) 

  print(ggplot(ans$site %>% select(site_name, latitude, longitude) %>%
           #filter(site_name %in% ans$site$site_name[1:8]) %>%
           mutate_at(vars(latitude, longitude), as.numeric)) + 
    borders("world", colour="gray80", fill="gray80") + # create a layer of borders
    geom_point(aes(x=longitude, y=latitude)) +
    geom_point(data=ans$site %>% select(site_name, latitude, longitude) %>%
           filter(site_name == oneSite) %>%
           mutate_at(vars(latitude, longitude), as.numeric),
                 aes(x=longitude[site_name==oneSite], y=latitude[site_name==oneSite]), color='red') +
    theme_bw() +
    theme(text=element_text(size=18),
          axis.title=element_blank()) +
    labs(title=oneSite))

}

ggplot(plot.df) +
  geom_histogram(aes(x=value)) +
  facet_wrap(~header+unit, scales='free') +
  labs(title='All C-PEAT Data')

ggplot(ans$site %>% select(site_name, latitude, longitude) %>%
         mutate_at(vars(latitude, longitude), as.numeric)) + 
  borders("world", colour="gray80", fill="gray80") + # create a layer of borders
  geom_point(aes(x=longitude, y=latitude), alpha=0.3) +
  theme_bw() +
  theme(text=element_text(size=18),
        axis.title=element_blank())

problematicLayers <- ans$layer %>% filter(layer_top > layer_bottom) %>%
  select(site_name) %>% unique

print(problematicLayers$site_name)
```
